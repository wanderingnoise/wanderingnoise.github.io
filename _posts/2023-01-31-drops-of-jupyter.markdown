---
layout: post
title:  "Drops of Jupyter"
date:   2023-01-31
description: Handle ML projects in Python with Jupyter
---

<p class="intro"><span class="dropcap">A</span>s you may have noticed, consistency is not my best soft skill. But, as a signal, I'm working hard to fight against noise in my head too.</p>

To be honest I worked a bit to found a balance between all my interests and I realized ML is not the only field I'd like to explore.
In fact, I've also started to speak a new language (a "nordic" one) thru dedicated lesson once per week and I'm approaching the finance fundamentals thru video courses and hard books.

That said, thanks to a friend suggestion, I stopped the Google ML crash course to start reading a well-written book called ![https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/]("Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow" by Aurélien Géron.

This book is openmind in terms of approach and well mix the theory and the practice that I desperately need. Highly recommended.
Furthemore, practical example are written in Jupyter.
I found tons of videos on the web based on Jupyter, that I never used and never was intended to use.

I thought of Jupyter as a graphical waste of time for almost 99% of projects.
BUT, what I found is Jupyter, in ML, makes sense.

Thanks to the plot libraries and the possibility to easily take a quick look at the data structure, it sounds such a good tool to include in your toolkit.

Coming back to ML deep dive, I only read the first 100 pages of the book in which ML is summarized and started to be explored, trying to follow the same "index" structure of the Google crash course (both were adherent).

A key concept I found is related to the **test dataset**. 

In fact, the book highlight the attention you need to have for creating a good test dataset not only extracting it as a portion of the whole dataset, but also focusing on preserving the percentage of every single element.

In a dataset, data is everything.

Data has to be cleaned up, standardized, and well represented, to find correlation.

I'm looking forward to including lot of examples in the following posts, hosted in the github repository, to show with practical example how to handle ML with Python to speed up (my) learning.
